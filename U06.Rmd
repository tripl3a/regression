---
title: "Regression - Exercises 6"
author: "Arndt Allhorn"
date: "June 7, 2018"
output: html_notebook
---

# Exercise 2

Generate artificial data (say a sample of size n = 100) from the N(1,4) distribution.

```{r}
set.seed(666)
N <- 100
obs <- rnorm(N, mean=1, sd=2)
mean(obs)
sd(obs)
```
```{r fig.width=4}
hist(obs)
```

Define the log-likelihood function.

```{r}
LL <- function(mu, sigma) {
     R = suppressWarnings(dnorm(obs, mu, sigma))
     return(-sum(log(R))) # negative b/c the stats4.mle() fn used below minimizes
 }
```


(a) Use the log-likelihood function from the previous Exercise. Plot it as a curve using the sample data and the known value σ²=4.

```{r}

wrapLL <- function(mu_vector,sigma) {
  result = rep(NA,length(mu_vector))
  for (i in 1:length(mu_vector)) {
    result[i] = LL(mu_vector[i], sigma)
  }
  return(result)
}
curve(wrapLL(x, 2)*-1, from=-2, to=4, main="log-likelihood")
```

(b) Optimize the log-likelihood numerically using R (check for example `?optimize`). Add the estimated value µ.hat to your plot to check if it is really at the maximum. (You may also add the true value µ=1.)

>Maximizing the log-likelihood of the parameters given a dataset is strictly equivalent as minimizing the negative log-likelihood. 

**Using `optimize`**

To find estimate for `mu` given `sigma`.

```{r}
mle.results <- optimize(function(mu) {LL(mu, sigma=2)},
                        interval = c(0, 2),
                        maximum = FALSE) # our LL returns negative values
 
mle.results
```

```{r}
curve(wrapLL(x, 2)*-1, from=-2, to=4, main="log-likelihood")
abline(v=mle.results$minimum, col="green")  #max. lik. estimate
abline(v=1, col="gray")   #true mean 
```

**Alternatively using `stats4.mle()`**

Apply MLE to estimate both parameters (mean and standard deviation) for which the normal distribution best describes the data.

```{r}
library(stats4)
mle(LL, start=list(mu=0.1, sigma=1))
```
  
A note of caution: if your initial guess for the parameters is too far off, then things can go seriously wrong! 

Sources:  
[Fitting a Model by Maximum Likelihood](https://datawookie.netlify.com/blog/2013/08/fitting-a-model-by-maximum-likelihood/)  
[ML notes: Why the log-likelihood?](https://blog.metaflow.fr/ml-notes-why-the-log-likelihood-24f7b6c40f83)

# Exercise 3

Important read: [Logistic regression: anova chi-square test vs. significance of coefficients (anova() vs summary() in R)](https://stats.stackexchange.com/questions/59879/logistic-regression-anova-chi-square-test-vs-significance-of-coefficients-ano))

* Running `anova(my.mod, test="Chisq")` __sequentially__ compares the smaller model with the next more complex model by adding one variable in each step.
* The p-values in the output of `summary(my.mod)` are Wald tests which test for each coefficient a model without that coefficient against the full model containing all coefficients (the order of the tests doesn't matter here).

```{r}
library(AER)
data(Affairs)

Affairs$affair=as.factor(as.numeric(Affairs$affairs>0))
table(Affairs$affair)

Affairs = Affairs[ ,!(names(Affairs) %in% c("affairs"))] # drop column

with(Affairs, spineplot(gender,affair))
with(Affairs, spineplot(age,affair))
with(Affairs, spineplot(yearsmarried,affair))

summary(Affairs)
glm1 = glm(affair~age, data=Affairs, family=binomial()) 
summary(glm1) # NOT significant
glm2 = glm(affair~yearsmarried, data=Affairs, family=binomial())
summary(glm2) # significant
glm3 = glm(affair~yearsmarried+children, data=Affairs, family=binomial())
summary(glm3) # only significant at 0.05 level
glm4 = glm(affair~yearsmarried+religiousness, data=Affairs, family=binomial())
summary(glm4) # both significant
glm5 = glm(affair~yearsmarried+religiousness+education, data=Affairs, family=binomial())
summary(glm5) # education NOT significant
glm6 = glm(affair~yearsmarried+religiousness+occupation, data=Affairs, family=binomial())
summary(glm6) # occupation NOT significant
glm7 = glm(affair~yearsmarried+religiousness+rating, data=Affairs, family=binomial())
summary(glm7) # all 3 siginificant
glm8 = glm(affair~yearsmarried+religiousness+rating+age, data=Affairs, family=binomial())
summary(glm8)
glmA = glm(affair~., data=Affairs, family=binomial())
summary(glmA)

# if not setting test="Chisq" parameter, anova just returns a analysis of deviance table for GLMs
anova(glm2, glm4, glm7, glm7, glmA)

# single model test
anova(glm4, test="Chisq")
anova(glm7, test="Chisq")
anova(glm8, test="Chisq")
anova(glmA, test="Chisq")
summary(glmA)

# model comparison
anova(glm2, glm4, test="Chisq")
anova(glm4, glm7, test="Chisq")
anova(glm4, glm7, glm8, glmA, test="Chisq") # glmA is not significantly better than glm8

# => glm8 seems to be the best

require(MASS)
stepAll <- stepAIC(glmA)
stepAll$anova # display results 
summary(stepAll)

# => confirms glm8 to be best
```







