---
title: "Regression - Exercises 4"
author: "Arndt Allhorn"
date: "May 24, 2018"
output: html_notebook
---

# Exercise 2

```{r setup, include=FALSE}
library(AER)
```

```{r}
data(CPS1985)
males <- CPS1985[CPS1985$gender=="male",]
females <- CPS1985[CPS1985$gender=="female",]
```
  
Estimate for both subsamples a multiple regression model for log(wage) on years of education, 
years of professional experience and squared experience. 
Consider the output from the respective summary for each of the subsamples:

```{r}
lmm = lm(log(wage)~ education + experience + I(experience^2), data=males)
summary(lmm)
```

```{r}
lmf = lm(log(wage)~ education + experience + I(experience^2), data=females)
summary(lmf)
```
#### (a) How could you determine the sample sizes of the two subsamples?

degrees of freedom + number of coefficients = sample size

#### (b) Which of the coefficients are sigficantly different from 0, if we assume a level of significance of 5%? 
(Does this change if we would use 1%?)

At alpha of 5% no coefficient is signifiant.
But at alpha of 1% the _I(experience^2)_ coefficient is siginificant. 

#### (c) How could you calculate the values of RSS for both models? Would it be useful to compare them?

To calculate RSS you have to sum up all the squared residuals: 
$$RSS = \sum(\hat y-y)^2$$

```{r}
sum(lmm$residuals^2)
deviance(lmm)
```
```{r}
sum(lmf$residuals^2)
deviance(lmf)
```

#### (d) Predict log(wage) for both models for a person with 12 years of education and 10 year of professional experience. What do you observe?

```{r}
predict(lmm, data.frame(education=12, experience=10))
predict(lmf, data.frame(education=12, experience=10))
```

There is a difference between females and males!

#### (e) Generate graphs for the marginal effects of experience for both models, i.e. display the estimated quadratic functions while setting education equal to 12 for example. (Note that 12 is the median of education in the full sample.)

```{r}
xrange <- range(males$experience, females$experience)
beta <- lmm$coefficients
curve(beta[1] + beta[2]*12 + beta[3]*x + beta[4]*x^2, lwd=2, xlim=xrange, col="red")
beta <- lmf$coefficients
curve(beta[1] + beta[2]*12 + beta[3]*x + beta[4]*x^2, lwd=2, add=T, col="blue")
legend("topleft",c("males","females"),lwd=2,col=c("red","blue"))
```

# Exercise 3

We generate artificial regression data:

```{r}
set.seed(123)
x <- runif(10)
y <- 2 - 2*x + 0.5*x^2 + rnorm(length(x), 0.2)
lm1 <- lm( y ~ x )
lm2 <- lm( y ~ x + I(x^2))
lm3 <- lm( y ~ x + I(x^2) + I(x^3) )
```

#### (a) Do a scatterplot of the data and graphically display the 3 estimated regression functions.

```{r}
plot(x,y)
curve(lm1$coefficients[1] + lm1$coefficients[2]*x, add=T, col="green", lwd=2)
curve(lm2$coefficients[1] + lm2$coefficients[2]*x + lm2$coefficients[3]*x^2, add=T, col="red", lwd=2)
curve(lm3$coefficients[1] + lm3$coefficients[2]*x + lm3$coefficients[3]*x^2 + lm3$coefficients[4]*x^3, add=T, col="blue", lwd=2)
legend("topright",c("linear","quadratic","cubic"),lwd=2,col=c("green","red","blue"))
```

#### (b) The R function `model.matrix` allows to extract the design matrix (X matrix) from an estimated regression model. Use this to calculate the hat matrices P1; P2; P3. Verify with R that all 3 matrices are projection matrices (which properties have to be checked?) and that their traces equal `p+1`.

```{r}
P1 = model.matrix(lm1); print(P1)
P2 = model.matrix(lm2); print(P2)
P3 = model.matrix(lm3); print(P3)
```

From the script:  

> Remark: P is a projection matrix as the matrix is symmetric and idempotent, i.e.  
> P^T = P and P·P=P^2=P  
> (the same does hold for I − P).

```{r}
sum(diag(P1)); sum(diag(P2)); sum(diag(P3))
```


#### (c) Do also verify with R that:
P2·P1=P1; P3·P1=P1 and P3·P2=P2

(Remark: For our models we have lm1 ⊆ lm2 ⊆ lm3. So, if we already projected
into the space spanned by the column vectors of a smaller design matrix, then the
projection on to a larger space does not change the result anymore.)

```{r}
# TODO
```

